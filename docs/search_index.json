[
["index.html", "Data 624 - HA Assignments (HW1) Chapter 1 Introduction", " Data 624 - HA Assignments (HW1) Group 1: Andrew Carson, Nathan Cooper, Baron Curtin, Heather Geiger 2019-03-23 Chapter 1 Introduction The work herein constitutes the work of Andrew Carson, Nathan Cooper, Baron Curtin, and Heather Geiger in the course Data 624 towards completion of the first set of homework assignments due for Professor Burk. "],
["hw1.html", "Chapter 2 HW1 2.1 2.3 2.2 2.7 2.3 2.10 2.4 3.1 2.5 3.8", " Chapter 2 HW1 2.1 2.3 2.1.1 a retailData &lt;- readxl::read_excel(&#39;./week1/retail.xlsx&#39;, skip=1) 2.1.2 b myts &lt;- ts(retailData[, &quot;A3349338X&quot;], frequency = 12, start=c(1982,4)) 2.1.3 c autoplot(myts[, &quot;A3349338X&quot;]) + ggtitle(&quot;Time Series Plot A3349338X&quot;) + xlab(&quot;Year&quot;) + ylab(&quot;Value&quot;) From the time series plot, we able to deduce: * Trend: there is a clear upward trend over the duration of the time series ggseasonplot(myts, year.labels = T) + ggtitle(&quot;Seasonal plot: A3349338X&quot;) ggsubseriesplot(myts) + ggtitle(&quot;Seasonal subseries pllot: A3349338X&quot;) From the seasonal plot, we are able to deduce: * Seasonality: there is some clear seasonality that exists among different month, however it is not without minor variation/deviation between the years * December shows a clear uptick while February shows a clear downtick ggAcf(myts) From the Acf plot, we are able to confirm: * Trend: the autocorrelations are slowly decreasing as the lags are increasing Conclusion: * Trend exists in this time series and is slowly going upwrd * Seasonality appears to exist however there are years that defy seasonality patterns * There does not appear to be any cyclic behavior 2.2 2.7 #arrivals &lt;- data(&quot;arrivals&quot;) autoplot(arrivals) + ggtitle(&quot;Arrivals Time Series&quot;) + facet_wrap(~series) Observations: * Japan seems to experience a an upward trend, reach an apex, and then experience a downward trend * NZ, UK, and US all experience upward trends in arrivals albeit to different degrees * UK appears to have the greatest variance in their arrival figures * US has very little deviation in their arrival trend * NZ has more fluctuation than US, but less than UK in their arrivals * US on average has the lowest amount of arrivals genSeasonPlot &lt;- function(country) { ggseasonplot(arrivals[, country]) + ggtitle(glue(&quot;Season Plot: {country}&quot;)) } countries &lt;- c(&quot;Japan&quot;, &quot;NZ&quot;, &quot;US&quot;, &quot;UK&quot;) countries %&gt;% map(~ genSeasonPlot(.x)) ## [[1]] ## ## [[2]] ## ## [[3]] ## ## [[4]] Observations from Seasonal Plots: * Typically the countries will see their highest arrivals in Q3, however the UK appears to go against this seasonal pattern * Q2 appears to be a down quarter for all countries except NZ * Q1 is a low point for NZ, and generally a high point for the other countries genSubSeasonPlot &lt;- function(country) { ggsubseriesplot(arrivals[, country]) + ggtitle(glue(&quot;Subseries Plot: {country}&quot;)) } countries %&gt;% map(~ genSubSeasonPlot(.x)) ## [[1]] ## ## [[2]] ## ## [[3]] ## ## [[4]] Observations: * Japan’s mean stays relatively flat across the quarters outside of Q2 * US also remains relatively consistent and further backs up the lack of deviation point made earlier * UK experiences the most arrivals in Q1 and A4 2.3 2.10 data(&quot;dj&quot;) ddj &lt;- diff(dj) autoplot(ddj) The autoplot appears to show random variation with peaks and valleys ggAcf(ddj) The changes in the Dow Jones Index do appear to be white noise as the the autocorrelations are very close to zero 2.4 3.1 2.4.1 usnetelec data(&quot;usnetelec&quot;) lambda &lt;- BoxCox.lambda(usnetelec) %&gt;% print() ## [1] 0.5167714 autoplot(BoxCox(usnetelec, lambda)) 2.4.2 usgdp data(&quot;usgdp&quot;) lambda &lt;- BoxCox.lambda(usgdp) %&gt;% print() ## [1] 0.366352 autoplot(BoxCox(usgdp, lambda)) 2.4.3 mcopper data(&quot;mcopper&quot;) lambda &lt;- BoxCox.lambda(mcopper) %&gt;% print() ## [1] 0.1919047 autoplot(BoxCox(mcopper, lambda)) 2.4.4 enplanements data(&quot;enplanements&quot;) lambda &lt;- BoxCox.lambda(enplanements) %&gt;% print() ## [1] -0.2269461 autoplot(BoxCox(enplanements, lambda)) 2.5 3.8 2.5.1 a myts.train &lt;- window(myts, end=c(2010,12)) myts.test &lt;- window(myts, start=2011) 2.5.2 b autoplot(myts) + autolayer(myts.train, series=&quot;Training&quot;) + autolayer(myts.test, series=&quot;Test&quot;) The autoplot shows that the data has been correctly split with the latter part of the data reservered for testing 2.5.3 c fc &lt;- snaive(myts.train) print(fc) ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## Jan 2011 255.4 219.8378 290.9622 201.0123 309.7877 ## Feb 2011 215.5 179.9378 251.0622 161.1123 269.8877 ## Mar 2011 229.6 194.0378 265.1622 175.2123 283.9877 ## Apr 2011 214.4 178.8378 249.9622 160.0123 268.7877 ## May 2011 224.5 188.9378 260.0622 170.1123 278.8877 ## Jun 2011 205.6 170.0378 241.1622 151.2123 259.9877 ## Jul 2011 198.7 163.1378 234.2622 144.3123 253.0877 ## Aug 2011 203.3 167.7378 238.8622 148.9123 257.6877 ## Sep 2011 203.2 167.6378 238.7622 148.8123 257.5877 ## Oct 2011 214.4 178.8378 249.9622 160.0123 268.7877 ## Nov 2011 201.4 165.8378 236.9622 147.0123 255.7877 ## Dec 2011 251.8 216.2378 287.3622 197.4123 306.1877 ## Jan 2012 255.4 205.1074 305.6926 178.4841 332.3159 ## Feb 2012 215.5 165.2074 265.7926 138.5841 292.4159 ## Mar 2012 229.6 179.3074 279.8926 152.6841 306.5159 ## Apr 2012 214.4 164.1074 264.6926 137.4841 291.3159 ## May 2012 224.5 174.2074 274.7926 147.5841 301.4159 ## Jun 2012 205.6 155.3074 255.8926 128.6841 282.5159 ## Jul 2012 198.7 148.4074 248.9926 121.7841 275.6159 ## Aug 2012 203.3 153.0074 253.5926 126.3841 280.2159 ## Sep 2012 203.2 152.9074 253.4926 126.2841 280.1159 ## Oct 2012 214.4 164.1074 264.6926 137.4841 291.3159 ## Nov 2012 201.4 151.1074 251.6926 124.4841 278.3159 ## Dec 2012 251.8 201.5074 302.0926 174.8841 328.7159 2.5.4 d accuracy(fc, myts.test) ## ME RMSE MAE MPE MAPE MASE ## Training set 5.502402 27.74935 19.33784 3.369450 10.447161 1.000000 ## Test set -10.845833 24.12202 18.52083 -5.910245 9.201624 0.957751 ## ACF1 Theil&#39;s U ## Training set 0.8703252 NA ## Test set 0.3564215 0.9855325 Conclusions: * The MASE shows that the seasonal naive method does produce a better forecast than the average naive forecast although very slightly. The MASE is only .95 * The MAPE shows that there is about a 9% error in the forecast on average which is not bad, but does show tht it could be improved * The RMSE and MAE both show that it is possible to improve the forecast but the seasonal naive forecast does do a decent job 2.5.5 e checkresiduals(fc) ## ## Ljung-Box test ## ## data: Residuals from Seasonal naive method ## Q* = 1256.8, df = 24, p-value &lt; 2.2e-16 ## ## Model df: 0. Total lags used: 24 The residual diagnostics show: * An approximately normal distribution * The mean of the residuals is close to 0 * The residual variance appears to be contant * The residuals appear to be correlated as the lags near to each other are similar in direction and size * Although the residuals pass the diagnostic tests, it does still show that the prediction intervals may be inaccurate * The Box-Ljung test does show a very small p-value which means the residuals are distinguishable from a white noise series. The Q* value is also very large 2.5.6 f Accuracy measures are very sensitive to the training/test split. Forecasts by definition need historical data. The more information present to forecast on, the better the forecast "],
["hw2.html", "Chapter 3 HW2 3.1 6.2 3.2 6.6", " Chapter 3 HW2 3.1 6.2 3.1.1 a autoplot(plastics) + xlab(&quot;Year&quot;) + ylab(&quot;1000 $&quot;) + ggtitle(&quot;Monthly Plastic Sales&quot;) ggsubseriesplot(plastics) + xlab(&quot;Month&quot;) + ylab(&quot;1000 $&quot;) The data shows a strong seasonal trend, with a peak in August or September and a trough in February every year. In addition there is an overall upward trend. 3.1.2 b plastics ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1 742 697 776 898 1030 1107 1165 1216 1208 1131 971 783 ## 2 741 700 774 932 1099 1223 1290 1349 1341 1296 1066 901 ## 3 896 793 885 1055 1204 1326 1303 1436 1473 1453 1170 1023 ## 4 951 861 938 1109 1274 1422 1486 1555 1604 1600 1403 1209 ## 5 1030 1032 1126 1285 1468 1637 1611 1608 1528 1420 1119 1013 autoplot(plastics) plas_trend &lt;- ma(plastics, order = 12, centre = TRUE) plas_trend ## Jan Feb Mar Apr May Jun Jul ## 1 NA NA NA NA NA NA 976.9583 ## 2 1000.4583 1011.2083 1022.2917 1034.7083 1045.5417 1054.4167 1065.7917 ## 3 1117.3750 1121.5417 1130.6667 1142.7083 1153.5833 1163.0000 1170.3750 ## 4 1208.7083 1221.2917 1231.7083 1243.2917 1259.1250 1276.5833 1287.6250 ## 5 1374.7917 1382.2083 1381.2500 1370.5833 1351.2500 1331.2500 NA ## Aug Sep Oct Nov Dec ## 1 977.0417 977.0833 978.4167 982.7083 990.4167 ## 2 1076.1250 1084.6250 1094.3750 1103.8750 1112.5417 ## 3 1175.5000 1180.5417 1185.0000 1190.1667 1197.0833 ## 4 1298.0417 1313.0000 1328.1667 1343.5833 1360.6250 ## 5 NA NA NA NA NA autoplot(plas_trend) plas_detrend &lt;- plastics/plas_trend plas_detrend ## Jan Feb Mar Apr May Jun Jul ## 1 NA NA NA NA NA NA 1.1924766 ## 2 0.7406605 0.6922411 0.7571225 0.9007369 1.0511298 1.1598830 1.2103679 ## 3 0.8018794 0.7070625 0.7827241 0.9232452 1.0437044 1.1401548 1.1133184 ## 4 0.7867903 0.7049913 0.7615439 0.8919870 1.0118138 1.1139108 1.1540627 ## 5 0.7492044 0.7466313 0.8152036 0.9375570 1.0864015 1.2296714 NA ## Aug Sep Oct Nov Dec ## 1 1.2445733 1.2363326 1.1559492 0.9880856 0.7905764 ## 2 1.2535718 1.2363720 1.1842376 0.9656890 0.8098573 ## 3 1.2216078 1.2477323 1.2261603 0.9830556 0.8545771 ## 4 1.1979585 1.2216299 1.2046681 1.0442225 0.8885622 ## 5 NA NA NA NA NA plas_seas &lt;- colMeans(t(matrix(plas_detrend,nrow = 12)), na.rm = TRUE) plas_seas ## [1] 0.7696337 0.7127315 0.7791485 0.9133815 1.0482624 1.1609050 1.1675564 ## [8] 1.2294279 1.2355167 1.1927538 0.9952632 0.8358933 autoplot(as.ts(plas_seas)) random_plas = plastics/ (plas_trend* plas_seas) (random_plas) ## Jan Feb Mar Apr May Jun Jul ## 1 NA NA NA NA NA NA 1.0213439 ## 2 0.9623546 0.9712509 0.9717306 0.9861563 1.0027354 0.9991197 1.0366676 ## 3 1.0418975 0.9920460 1.0045890 1.0107991 0.9956519 0.9821258 0.9535457 ## 4 1.0222920 0.9891400 0.9774053 0.9765766 0.9652295 0.9595194 0.9884428 ## 5 0.9734559 1.0475631 1.0462750 1.0264681 1.0363832 1.0592351 NA ## Aug Sep Oct Nov Dec ## 1 1.0123191 1.0006604 0.9691432 0.9927883 0.9457863 ## 2 1.0196384 1.0006923 0.9928600 0.9702851 0.9688526 ## 3 0.9936393 1.0098871 1.0280079 0.9877343 1.0223520 ## 4 0.9744032 0.9887603 1.0099889 1.0491923 1.0630092 ## 5 NA NA NA NA NA random_plas ## Jan Feb Mar Apr May Jun Jul ## 1 NA NA NA NA NA NA 1.0213439 ## 2 0.9623546 0.9712509 0.9717306 0.9861563 1.0027354 0.9991197 1.0366676 ## 3 1.0418975 0.9920460 1.0045890 1.0107991 0.9956519 0.9821258 0.9535457 ## 4 1.0222920 0.9891400 0.9774053 0.9765766 0.9652295 0.9595194 0.9884428 ## 5 0.9734559 1.0475631 1.0462750 1.0264681 1.0363832 1.0592351 NA ## Aug Sep Oct Nov Dec ## 1 1.0123191 1.0006604 0.9691432 0.9927883 0.9457863 ## 2 1.0196384 1.0006923 0.9928600 0.9702851 0.9688526 ## 3 0.9936393 1.0098871 1.0280079 0.9877343 1.0223520 ## 4 0.9744032 0.9887603 1.0099889 1.0491923 1.0630092 ## 5 NA NA NA NA NA plastics %&gt;% decompose(type=&quot;multiplicative&quot;) %&gt;% autoplot() + xlab(&quot;Year&quot;) + ggtitle(&quot;Classical multiplicative decomposition of Plastic Sales&quot;) plastics %&gt;% decompose(type=&quot;multiplicative&quot;) ## $x ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1 742 697 776 898 1030 1107 1165 1216 1208 1131 971 783 ## 2 741 700 774 932 1099 1223 1290 1349 1341 1296 1066 901 ## 3 896 793 885 1055 1204 1326 1303 1436 1473 1453 1170 1023 ## 4 951 861 938 1109 1274 1422 1486 1555 1604 1600 1403 1209 ## 5 1030 1032 1126 1285 1468 1637 1611 1608 1528 1420 1119 1013 ## ## $seasonal ## Jan Feb Mar Apr May Jun Jul ## 1 0.7670466 0.7103357 0.7765294 0.9103112 1.0447386 1.1570026 1.1636317 ## 2 0.7670466 0.7103357 0.7765294 0.9103112 1.0447386 1.1570026 1.1636317 ## 3 0.7670466 0.7103357 0.7765294 0.9103112 1.0447386 1.1570026 1.1636317 ## 4 0.7670466 0.7103357 0.7765294 0.9103112 1.0447386 1.1570026 1.1636317 ## 5 0.7670466 0.7103357 0.7765294 0.9103112 1.0447386 1.1570026 1.1636317 ## Aug Sep Oct Nov Dec ## 1 1.2252952 1.2313635 1.1887444 0.9919176 0.8330834 ## 2 1.2252952 1.2313635 1.1887444 0.9919176 0.8330834 ## 3 1.2252952 1.2313635 1.1887444 0.9919176 0.8330834 ## 4 1.2252952 1.2313635 1.1887444 0.9919176 0.8330834 ## 5 1.2252952 1.2313635 1.1887444 0.9919176 0.8330834 ## ## $trend ## Jan Feb Mar Apr May Jun Jul ## 1 NA NA NA NA NA NA 976.9583 ## 2 1000.4583 1011.2083 1022.2917 1034.7083 1045.5417 1054.4167 1065.7917 ## 3 1117.3750 1121.5417 1130.6667 1142.7083 1153.5833 1163.0000 1170.3750 ## 4 1208.7083 1221.2917 1231.7083 1243.2917 1259.1250 1276.5833 1287.6250 ## 5 1374.7917 1382.2083 1381.2500 1370.5833 1351.2500 1331.2500 NA ## Aug Sep Oct Nov Dec ## 1 977.0417 977.0833 978.4167 982.7083 990.4167 ## 2 1076.1250 1084.6250 1094.3750 1103.8750 1112.5417 ## 3 1175.5000 1180.5417 1185.0000 1190.1667 1197.0833 ## 4 1298.0417 1313.0000 1328.1667 1343.5833 1360.6250 ## 5 NA NA NA NA NA ## ## $random ## Jan Feb Mar Apr May Jun Jul ## 1 NA NA NA NA NA NA 1.0247887 ## 2 0.9656005 0.9745267 0.9750081 0.9894824 1.0061175 1.0024895 1.0401641 ## 3 1.0454117 0.9953920 1.0079773 1.0142083 0.9990100 0.9854384 0.9567618 ## 4 1.0257400 0.9924762 0.9807020 0.9798704 0.9684851 0.9627557 0.9917766 ## 5 0.9767392 1.0510964 1.0498039 1.0299302 1.0398787 1.0628077 NA ## Aug Sep Oct Nov Dec ## 1 1.0157335 1.0040354 0.9724119 0.9961368 0.9489762 ## 2 1.0230774 1.0040674 0.9962088 0.9735577 0.9721203 ## 3 0.9969907 1.0132932 1.0314752 0.9910657 1.0258002 ## 4 0.9776897 0.9920952 1.0133954 1.0527311 1.0665946 ## 5 NA NA NA NA NA ## ## $figure ## [1] 0.7670466 0.7103357 0.7765294 0.9103112 1.0447386 1.1570026 1.1636317 ## [8] 1.2252952 1.2313635 1.1887444 0.9919176 0.8330834 ## ## $type ## [1] &quot;multiplicative&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;decomposed.ts&quot; 3.1.3 c Yes the trend is sloped upward and the seasonal trend reaches maximum once a year. 3.1.4 d plas_seas &lt;- decompose(plastics, type = &#39;multiplicative&#39;)$seasonal plas_seas ## Jan Feb Mar Apr May Jun Jul ## 1 0.7670466 0.7103357 0.7765294 0.9103112 1.0447386 1.1570026 1.1636317 ## 2 0.7670466 0.7103357 0.7765294 0.9103112 1.0447386 1.1570026 1.1636317 ## 3 0.7670466 0.7103357 0.7765294 0.9103112 1.0447386 1.1570026 1.1636317 ## 4 0.7670466 0.7103357 0.7765294 0.9103112 1.0447386 1.1570026 1.1636317 ## 5 0.7670466 0.7103357 0.7765294 0.9103112 1.0447386 1.1570026 1.1636317 ## Aug Sep Oct Nov Dec ## 1 1.2252952 1.2313635 1.1887444 0.9919176 0.8330834 ## 2 1.2252952 1.2313635 1.1887444 0.9919176 0.8330834 ## 3 1.2252952 1.2313635 1.1887444 0.9919176 0.8330834 ## 4 1.2252952 1.2313635 1.1887444 0.9919176 0.8330834 ## 5 1.2252952 1.2313635 1.1887444 0.9919176 0.8330834 plas_seas_adj = plastics/plas_seas autoplot(plas_seas_adj) 3.1.5 e plastics[30] ## [1] 1326 new_val &lt;- plastics[30]+500 plas_outlier &lt;- replace(plastics,30, new_val) plas_outlier[30] ## [1] 1826 plas_seas_outlier &lt;- decompose(plas_outlier, type = &#39;multiplicative&#39;)$seasonal plas_seas_outlier ## Jan Feb Mar Apr May Jun Jul ## 1 0.7598996 0.7040581 0.7696340 0.9022607 1.0357218 1.2506498 1.1541497 ## 2 0.7598996 0.7040581 0.7696340 0.9022607 1.0357218 1.2506498 1.1541497 ## 3 0.7598996 0.7040581 0.7696340 0.9022607 1.0357218 1.2506498 1.1541497 ## 4 0.7598996 0.7040581 0.7696340 0.9022607 1.0357218 1.2506498 1.1541497 ## 5 0.7598996 0.7040581 0.7696340 0.9022607 1.0357218 1.2506498 1.1541497 ## Aug Sep Oct Nov Dec ## 1 1.2149325 1.2208222 1.1784222 0.9836787 0.8257707 ## 2 1.2149325 1.2208222 1.1784222 0.9836787 0.8257707 ## 3 1.2149325 1.2208222 1.1784222 0.9836787 0.8257707 ## 4 1.2149325 1.2208222 1.1784222 0.9836787 0.8257707 ## 5 1.2149325 1.2208222 1.1784222 0.9836787 0.8257707 plas_seas_adj_outlier = plas_outlier/plas_seas_outlier autoplot(plas_seas_adj_outlier) It adds a spike to the seasonally adjusted data where the outlier is. 3.1.6 f plastics[5] ## [1] 1030 new_val &lt;- plastics[5]+500 plas_outlier &lt;- replace(plastics,5, new_val) plas_outlier[5] ## [1] 1530 plas_seas_outlier &lt;- decompose(plas_outlier, type = &#39;multiplicative&#39;)$seasonal plas_seas_outlier ## Jan Feb Mar Apr May Jun Jul ## 1 0.7705339 0.7135653 0.7800599 0.9144500 1.0494886 1.1622630 1.1567134 ## 2 0.7705339 0.7135653 0.7800599 0.9144500 1.0494886 1.1622630 1.1567134 ## 3 0.7705339 0.7135653 0.7800599 0.9144500 1.0494886 1.1622630 1.1567134 ## 4 0.7705339 0.7135653 0.7800599 0.9144500 1.0494886 1.1622630 1.1567134 ## 5 0.7705339 0.7135653 0.7800599 0.9144500 1.0494886 1.1622630 1.1567134 ## Aug Sep Oct Nov Dec ## 1 1.2181249 1.2243057 1.1823311 0.9912933 0.8368710 ## 2 1.2181249 1.2243057 1.1823311 0.9912933 0.8368710 ## 3 1.2181249 1.2243057 1.1823311 0.9912933 0.8368710 ## 4 1.2181249 1.2243057 1.1823311 0.9912933 0.8368710 ## 5 1.2181249 1.2243057 1.1823311 0.9912933 0.8368710 plas_seas_adj_outlier = plas_outlier/plas_seas_outlier autoplot(plas_seas_adj_outlier) plastics[55] ## [1] 1611 new_val &lt;- plastics[55]+500 plas_outlier &lt;- replace(plastics,55, new_val) plas_outlier[55] ## [1] 2111 plas_seas_outlier &lt;- decompose(plas_outlier, type = &#39;multiplicative&#39;)$seasonal plas_seas_outlier ## Jan Feb Mar Apr May Jun Jul ## 1 0.7667177 0.7071587 0.7730597 0.9063243 1.0399751 1.1513949 1.1673737 ## 2 0.7667177 0.7071587 0.7730597 0.9063243 1.0399751 1.1513949 1.1673737 ## 3 0.7667177 0.7071587 0.7730597 0.9063243 1.0399751 1.1513949 1.1673737 ## 4 0.7667177 0.7071587 0.7730597 0.9063243 1.0399751 1.1513949 1.1673737 ## 5 0.7667177 0.7071587 0.7730597 0.9063243 1.0399751 1.1513949 1.1673737 ## Aug Sep Oct Nov Dec ## 1 1.2292355 1.2353233 1.1925671 0.9951074 0.8357624 ## 2 1.2292355 1.2353233 1.1925671 0.9951074 0.8357624 ## 3 1.2292355 1.2353233 1.1925671 0.9951074 0.8357624 ## 4 1.2292355 1.2353233 1.1925671 0.9951074 0.8357624 ## 5 1.2292355 1.2353233 1.1925671 0.9951074 0.8357624 plas_seas_adj_outlier = plas_outlier/plas_seas_outlier autoplot(plas_seas_adj_outlier) Yes, the spike occurs where the outlier is. 3.2 6.6 3.2.1 a bricksq %&gt;% autoplot() bricksq %&gt;% stl(t.window=13, s.window=&quot;periodic&quot;, robust=TRUE) %&gt;% autoplot() bricksq %&gt;% stl(t.window=13, s.window=7, robust=TRUE) %&gt;% autoplot() bricksq %&gt;% stl(t.window=13, s.window=25, robust=TRUE) %&gt;% autoplot() bricksq %&gt;% stl(t.window=13, s.window=81, robust=TRUE) %&gt;% autoplot() We see that a high s.window yeilds the same as periodic. bricksq %&gt;% autoplot() bricksq %&gt;% stl(t.window=13, s.window=&quot;periodic&quot;, robust=TRUE) %&gt;% autoplot() bricksq %&gt;% stl(t.window=25, s.window=&quot;periodic&quot;, robust=TRUE) %&gt;% autoplot() bricksq %&gt;% stl(t.window=7, s.window=&quot;periodic&quot;, robust=TRUE) %&gt;% autoplot() bricksq %&gt;% stl(t.window=3, s.window=&quot;periodic&quot;, robust=TRUE) %&gt;% autoplot() The t.window parameter adjusts the goodness of fit of the trend, low values tend to under fit the trend, and high values tend to over fit the trend. 3.2.2 b brick_seas &lt;- decompose(bricksq, type = &#39;multiplicative&#39;)$seasonal brick_seas ## Qtr1 Qtr2 Qtr3 Qtr4 ## 1956 0.907788 1.027191 1.061155 1.003866 ## 1957 0.907788 1.027191 1.061155 1.003866 ## 1958 0.907788 1.027191 1.061155 1.003866 ## 1959 0.907788 1.027191 1.061155 1.003866 ## 1960 0.907788 1.027191 1.061155 1.003866 ## 1961 0.907788 1.027191 1.061155 1.003866 ## 1962 0.907788 1.027191 1.061155 1.003866 ## 1963 0.907788 1.027191 1.061155 1.003866 ## 1964 0.907788 1.027191 1.061155 1.003866 ## 1965 0.907788 1.027191 1.061155 1.003866 ## 1966 0.907788 1.027191 1.061155 1.003866 ## 1967 0.907788 1.027191 1.061155 1.003866 ## 1968 0.907788 1.027191 1.061155 1.003866 ## 1969 0.907788 1.027191 1.061155 1.003866 ## 1970 0.907788 1.027191 1.061155 1.003866 ## 1971 0.907788 1.027191 1.061155 1.003866 ## 1972 0.907788 1.027191 1.061155 1.003866 ## 1973 0.907788 1.027191 1.061155 1.003866 ## 1974 0.907788 1.027191 1.061155 1.003866 ## 1975 0.907788 1.027191 1.061155 1.003866 ## 1976 0.907788 1.027191 1.061155 1.003866 ## 1977 0.907788 1.027191 1.061155 1.003866 ## 1978 0.907788 1.027191 1.061155 1.003866 ## 1979 0.907788 1.027191 1.061155 1.003866 ## 1980 0.907788 1.027191 1.061155 1.003866 ## 1981 0.907788 1.027191 1.061155 1.003866 ## 1982 0.907788 1.027191 1.061155 1.003866 ## 1983 0.907788 1.027191 1.061155 1.003866 ## 1984 0.907788 1.027191 1.061155 1.003866 ## 1985 0.907788 1.027191 1.061155 1.003866 ## 1986 0.907788 1.027191 1.061155 1.003866 ## 1987 0.907788 1.027191 1.061155 1.003866 ## 1988 0.907788 1.027191 1.061155 1.003866 ## 1989 0.907788 1.027191 1.061155 1.003866 ## 1990 0.907788 1.027191 1.061155 1.003866 ## 1991 0.907788 1.027191 1.061155 1.003866 ## 1992 0.907788 1.027191 1.061155 1.003866 ## 1993 0.907788 1.027191 1.061155 1.003866 ## 1994 0.907788 1.027191 1.061155 brick_seas_adj = bricksq/brick_seas autoplot(brick_seas_adj) 3.2.3 c brick_naive &lt;- naive(brick_seas_adj, h=12) brick_naive ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 1994 Q4 465.5304 438.0189 493.0419 423.4551 507.6056 ## 1995 Q1 465.5304 426.6232 504.4375 406.0270 525.0337 ## 1995 Q2 465.5304 417.8790 513.1817 392.6539 538.4068 ## 1995 Q3 465.5304 410.5073 520.5534 381.3799 549.6808 ## 1995 Q4 465.5304 404.0128 527.0480 371.4473 559.6134 ## 1996 Q1 465.5304 398.1412 532.9195 362.4675 568.5932 ## 1996 Q2 465.5304 392.7417 538.3190 354.2098 576.8509 ## 1996 Q3 465.5304 387.7161 543.3447 346.5237 584.5371 ## 1996 Q4 465.5304 382.9958 548.0649 339.3047 591.7560 ## 1997 Q1 465.5304 378.5313 552.5294 332.4768 598.5839 ## 1997 Q2 465.5304 374.2850 556.7757 325.9826 605.0781 ## 1997 Q3 465.5304 370.2277 560.8330 319.7775 611.2832 autoplot(brick_seas_adj) + autolayer(brick_naive, series=&quot;Na�ve&quot;, PI=TRUE) 3.2.4 d fcast &lt;- stlf(brick_seas_adj, method=&#39;naive&#39;) autoplot(fcast) 3.2.5 e res_brk &lt;- residuals(brick_naive) autoplot(res_brk) gghistogram(res_brk) + ggtitle(&quot;Histogram of residuals&quot;) ggAcf(res_brk) + ggtitle(&quot;ACF of residuals&quot;) res_fcast &lt;- residuals(fcast) autoplot(res_fcast) gghistogram(res_brk) + ggtitle(&quot;Histogram of residuals&quot;) They seem to increse as time increases, so no they do not look uncorrelated. Also a couple of the lags show significance, that may or may not be by chance. 3.2.6 f brick_stl &lt;- brick_seas_adj %&gt;% stlf(t.window=7, s.window=&quot;periodic&quot;, method = &#39;naive&#39;, robust=TRUE) brick_stl %&gt;% autoplot() In this instance, it the residuals appear to behave similarly to a STL decompostion that is not robust. res_brk2 &lt;- brick_stl %&gt;% residuals() res_brk2 %&gt;% autoplot() gghistogram(res_brk2) + ggtitle(&quot;Histogram of residuals&quot;) ggAcf(res_brk2) + ggtitle(&quot;ACF of residuals&quot;) 3.2.7 g train &lt;- bricksq %&gt;% window(1956, c(1992, 3)) test &lt;- bricksq %&gt;% window(c(1992, 4), c(1994, 4)) train_stlf &lt;- train %&gt;% stlf(method=&#39;naive&#39;, h=8) train_snaiv &lt;- train %&gt;% snaive(h=8) autoplot(bricksq) + autolayer(train_stlf, series=&quot;STL&quot;, PI=FALSE) + autolayer(train_snaiv, series=&quot;Seasonal na�ve&quot;, PI=FALSE) + xlab(&quot;Quarter&quot;) + ylab(&quot;Bricks&quot;) + ggtitle(&quot;Forecasts for quarterly brick production&quot;) + guides(colour=guide_legend(title=&quot;Forecast&quot;)) From the graphs, it is hard to tell. accuracy(train_snaiv, test) ## ME RMSE MAE MPE MAPE MASE ## Training set 6.174825 49.71281 36.41259 1.369661 8.903098 1.0000000 ## Test set 27.500000 35.05353 30.00000 5.933607 6.528845 0.8238909 ## ACF1 Theil&#39;s U ## Training set 0.8105927 NA ## Test set 0.2405423 0.9527794 accuracy(train_stlf, test) ## ME RMSE MAE MPE MAPE MASE ## Training set 1.457806 20.31551 14.65964 0.3594803 3.606373 0.4025982 ## Test set 23.795651 27.73302 24.77223 5.2310577 5.463576 0.6803205 ## ACF1 Theil&#39;s U ## Training set 0.2005515 NA ## Test set 0.2530477 0.7247275 From the accuracy fucnction, it appears that the STL decomposition performs better by yeilding less error in ME, RMSE, MAE, MPE, MAPE, and MASE. "],
["hw3.html", "Chapter 4 HW3 4.1 3.1 4.2 3.2", " Chapter 4 HW3 4.1 3.1 4.1.1 3.1a Answer: I use histograms to understand the distribution of each predictor variable. The variables differ quite a bit. Some are more normally distributed (e.g., Na, Al) while others do not look normal at all (e.g., Ba, Fe, K). #predictor distributions. examples. for(i in c(2, 6, 9)){ hist(Glass[,i], main = names(Glass)[i], xlab = names(Glass)[i], breaks = 20) } I use a correlation plot to help me understand the correlations between predictors. There are some strong positive relationships (i.e., Rl and Ca, Al and Ba) as well as some strong negative relationships (i.e., Rl and Si, Rl and Al, Mg and Ba). Most relationships are not very strong. #relationships between predictors loadPkg(&quot;corrplot&quot;) correlations &lt;- cor(Glass[,1:9]) corrplot(correlations) 4.1.2 3.1b Answer: Yes, there do appear to be outliers. “K” has a very obvious outlier with a value of 6. “Ba” also has outliers at above 2.0,and “Fe” has an outlier above 0.5. Skew is also present in many predictors. While some have only minor skew (e.g., Rl, Al), others are much more pronounced and obvious (e.g., Mg, Ba, Fe) 4.1.3 3.1c Answer: Yes, a log or Box Cox transformation could help remove the skew mentioned above. Depending on what kind of classification model we are using, centering and scaling could be important for all variables. For example, a logistic regression classification type model will be much more sensitive to variables on different scales than a decision tree. Removing the outliers may still be required after addressing skew, so that may be needed as well to improve model performance. Thankfully, there are no missing values in any columns, so we do not need to address those by imputation, removal, or other means. 4.2 3.2 loadPkg(&quot;mlbench&quot;) data(Soybean) 4.2.1 3.2a Answer: The frequency distributions for the categorical predictors are degenerate. Most consist of two or three values, so the distributions are not normal. This is not surprising as the values are categorical, not continuous. Often the values in the variables are not evenly distributed by frequency, with one or more values having a much greater frequency than others (e.g., leaf.marg has many 0 and 2 but few 1). #frequency distributions. examples for(i in c(4,13,15)){ plot(Soybean[,i], main = names(Soybean)[i]) } There are lots of missing values in the data set as a whole, and nearly every variable has missing values. #summary. show missing value counts summary(Soybean[,2:36]) ## date plant.stand precip temp hail crop.hist ## 5 :149 0 :354 0 : 74 0 : 80 0 :435 0 : 65 ## 4 :131 1 :293 1 :112 1 :374 1 :127 1 :165 ## 3 :118 NA&#39;s: 36 2 :459 2 :199 NA&#39;s:121 2 :219 ## 2 : 93 NA&#39;s: 38 NA&#39;s: 30 3 :218 ## 6 : 90 NA&#39;s: 16 ## (Other):101 ## NA&#39;s : 1 ## area.dam sever seed.tmt germ plant.growth leaves ## 0 :123 0 :195 0 :305 0 :165 0 :441 0: 77 ## 1 :227 1 :322 1 :222 1 :213 1 :226 1:606 ## 2 :145 2 : 45 2 : 35 2 :193 NA&#39;s: 16 ## 3 :187 NA&#39;s:121 NA&#39;s:121 NA&#39;s:112 ## NA&#39;s: 1 ## ## ## leaf.halo leaf.marg leaf.size leaf.shread leaf.malf leaf.mild ## 0 :221 0 :357 0 : 51 0 :487 0 :554 0 :535 ## 1 : 36 1 : 21 1 :327 1 : 96 1 : 45 1 : 20 ## 2 :342 2 :221 2 :221 NA&#39;s:100 NA&#39;s: 84 2 : 20 ## NA&#39;s: 84 NA&#39;s: 84 NA&#39;s: 84 NA&#39;s:108 ## ## ## ## stem lodging stem.cankers canker.lesion fruiting.bodies ## 0 :296 0 :520 0 :379 0 :320 0 :473 ## 1 :371 1 : 42 1 : 39 1 : 83 1 :104 ## NA&#39;s: 16 NA&#39;s:121 2 : 36 2 :177 NA&#39;s:106 ## 3 :191 3 : 65 ## NA&#39;s: 38 NA&#39;s: 38 ## ## ## ext.decay mycelium int.discolor sclerotia fruit.pods fruit.spots ## 0 :497 0 :639 0 :581 0 :625 0 :407 0 :345 ## 1 :135 1 : 6 1 : 44 1 : 20 1 :130 1 : 75 ## 2 : 13 NA&#39;s: 38 2 : 20 NA&#39;s: 38 2 : 14 2 : 57 ## NA&#39;s: 38 NA&#39;s: 38 3 : 48 4 :100 ## NA&#39;s: 84 NA&#39;s:106 ## ## ## seed mold.growth seed.discolor seed.size shriveling roots ## 0 :476 0 :524 0 :513 0 :532 0 :539 0 :551 ## 1 :115 1 : 67 1 : 64 1 : 59 1 : 38 1 : 86 ## NA&#39;s: 92 NA&#39;s: 92 NA&#39;s:106 NA&#39;s: 92 NA&#39;s:106 2 : 15 ## NA&#39;s: 31 ## ## ## Furthermore, once the variables are transformed into dummy variables, there are clear cases of collinearity (e.g., roots value “2” has 0.96 correlation with fruit pods value “2”, shriveling value “1” has 0.86 correlation with seed size value “1”). So the data has lots of issues from a modeling perspective. #collinearity loadPkg(&quot;caret&quot;) soy_dummy_model &lt;-dummyVars(~., data=Soybean[,2:36]) soy_dummy &lt;-predict(soy_dummy_model, Soybean[,2:36]) soy_dummy&lt;-data.frame(soy_dummy) #correlation plot corr_soy&lt;- cor(soy_dummy, use = &quot;pairwise.complete.obs&quot;) corrplot(corr_soy) 4.2.2 3.2b Answer: Yes, there are particular predictors that are more likely to be missing. A count of NAs below shows that we have counts of NAs in each column ranging from 0 through 121. A distribution of the NA counts shows that it not normally distributed, and that there are gaps in the middle of the range (40 to 80). loadPkg(&quot;dplyr&quot;) #get counts of NAs df_na &lt;-c() for(i in 2:36){ name &lt;- names(Soybean)[i] count &lt;-sum(is.na(Soybean[,i])) row &lt;-c(name, count) df_na &lt;- rbind(df_na, row) } df_na &lt;- data.frame(df_na, row.names = NULL, stringsAsFactors = FALSE) names(df_na)&lt;-c(&quot;Variable&quot;, &quot;NA_Count&quot;) df_na$NA_Count &lt;- as.integer(df_na$NA_Count) head(arrange(df_na, desc(NA_Count)), n=10) ## Variable NA_Count ## 1 hail 121 ## 2 sever 121 ## 3 seed.tmt 121 ## 4 lodging 121 ## 5 germ 112 ## 6 leaf.mild 108 ## 7 fruiting.bodies 106 ## 8 fruit.spots 106 ## 9 seed.discolor 106 ## 10 shriveling 106 #hist hist(df_na$NA_Count, breaks = 10) Is the pattern of missing data related to the classes? There are 19 classes, and there is definitely a pattern related to the classes. The classes below have repeated counts of missing values across multiple variables: 2-4-d-injury: 16 cyst-nematode: 14 diaporthe-pod-&amp;-stem-blight: 15 herbicide-injury: 8 phytophthora-rotL: 68 or 55 #get NAs by class Soybean_NA &lt;-is.na(Soybean) Soybean_NA[Soybean_NA==TRUE] &lt;-1 Soybean_NA &lt;- data.frame(Soybean_NA) Soybean_NA$Class &lt;- as.character(Soybean$Class) #group to get count of NAs per column by class value Soybean_NA_grouped &lt;- Soybean_NA %&gt;% group_by(Class) %&gt;% summarise_all(funs(sum(.))) Soybean_NA_grouped ## # A tibble: 19 x 36 ## Class date plant.stand precip temp hail crop.hist area.dam sever ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2-4-~ 1 16 16 16 16 16 1 16 ## 2 alte~ 0 0 0 0 0 0 0 0 ## 3 anth~ 0 0 0 0 0 0 0 0 ## 4 bact~ 0 0 0 0 0 0 0 0 ## 5 bact~ 0 0 0 0 0 0 0 0 ## 6 brow~ 0 0 0 0 0 0 0 0 ## 7 brow~ 0 0 0 0 0 0 0 0 ## 8 char~ 0 0 0 0 0 0 0 0 ## 9 cyst~ 0 14 14 14 14 0 0 14 ## 10 diap~ 0 6 0 0 15 0 0 15 ## 11 diap~ 0 0 0 0 0 0 0 0 ## 12 down~ 0 0 0 0 0 0 0 0 ## 13 frog~ 0 0 0 0 0 0 0 0 ## 14 herb~ 0 0 8 0 8 0 0 8 ## 15 phyl~ 0 0 0 0 0 0 0 0 ## 16 phyt~ 0 0 0 0 68 0 0 68 ## 17 powd~ 0 0 0 0 0 0 0 0 ## 18 purp~ 0 0 0 0 0 0 0 0 ## 19 rhiz~ 0 0 0 0 0 0 0 0 ## # ... with 27 more variables: seed.tmt &lt;dbl&gt;, germ &lt;dbl&gt;, ## # plant.growth &lt;dbl&gt;, leaves &lt;dbl&gt;, leaf.halo &lt;dbl&gt;, leaf.marg &lt;dbl&gt;, ## # leaf.size &lt;dbl&gt;, leaf.shread &lt;dbl&gt;, leaf.malf &lt;dbl&gt;, leaf.mild &lt;dbl&gt;, ## # stem &lt;dbl&gt;, lodging &lt;dbl&gt;, stem.cankers &lt;dbl&gt;, canker.lesion &lt;dbl&gt;, ## # fruiting.bodies &lt;dbl&gt;, ext.decay &lt;dbl&gt;, mycelium &lt;dbl&gt;, ## # int.discolor &lt;dbl&gt;, sclerotia &lt;dbl&gt;, fruit.pods &lt;dbl&gt;, ## # fruit.spots &lt;dbl&gt;, seed &lt;dbl&gt;, mold.growth &lt;dbl&gt;, seed.discolor &lt;dbl&gt;, ## # seed.size &lt;dbl&gt;, shriveling &lt;dbl&gt;, roots &lt;dbl&gt; In fact, these 5 classes are the only classes that have missing values. #calculate totals of NAs by class totals &lt;-rowSums(Soybean_NA_grouped[,2:36]) Soybean_NA_totals &lt;- data.frame(cbind(Soybean_NA_grouped$Class, totals), stringsAsFactors = FALSE) names(Soybean_NA_totals) &lt;-c(&quot;Class&quot;,&quot;Totals&quot;) Soybean_NA_totals$Totals &lt;- as.integer(Soybean_NA_totals$Totals) head(arrange(Soybean_NA_totals, desc(Totals)), n=10) ## Class Totals ## 1 phytophthora-rot 1214 ## 2 2-4-d-injury 450 ## 3 cyst-nematode 336 ## 4 diaporthe-pod-&amp;-stem-blight 177 ## 5 herbicide-injury 160 ## 6 alternarialeaf-spot 0 ## 7 anthracnose 0 ## 8 bacterial-blight 0 ## 9 bacterial-pustule 0 ## 10 brown-spot 0 4.2.3 3.2c Answer: Since missing values are related to particular classes, we do not want to remove rows with missing values, as this would be to remove information that could predict the appropriate class. Furthermore, if we did that, we might remove all or nearly all of the rows that have a particular class. For example, there are 16 rows with a class of “2-4-d-injury”. And there are 16 rows that are missing values in the “plant stand” column that have a class of “2-4-d-injury”. So if we removed all of these, we would be removing every instance of “2-4-d-injury”, which we certainly do not want to do. #counts of rows by class summary(Soybean$Class) ## 2-4-d-injury alternarialeaf-spot ## 16 91 ## anthracnose bacterial-blight ## 44 20 ## bacterial-pustule brown-spot ## 20 92 ## brown-stem-rot charcoal-rot ## 44 20 ## cyst-nematode diaporthe-pod-&amp;-stem-blight ## 14 15 ## diaporthe-stem-canker downy-mildew ## 20 20 ## frog-eye-leaf-spot herbicide-injury ## 91 8 ## phyllosticta-leaf-spot phytophthora-rot ## 20 88 ## powdery-mildew purple-seed-stain ## 20 20 ## rhizoctonia-root-rot ## 20 I would not want to remove predictors either, as all of the predictors have some number of missing values. The only case in which I would be comfortable with removing predictors would be if predictors were highly correlated and I could remove one without losing information. However, the remaining predictor would likely still have lots of missing values that would need to be addressed. Consequently, I would do two things. First, I would make additional predictors based on whether a value was missing or not for each original predictor. For example, I would add a “Hail_NA” column which would have a 1 if the value was missing and a 0 if it was not for every row in the data set. The addition of such predictors would be very useful in a decision tree model, as the presence of any missing values reduces the class options down to 5, and a split on having missing hail values but no missing precip values reduces the options to 2, and finally, a split on no missing plant stand values reduces the options to 1. Such additional NA predictor columns may prove useful for a decision tree model, but are likely less important for a logistic regression classifier. Thus, I would want to handle the missing values using imputation as well, as this approach will work better for logistic regression models. As mentioned in the book, a kNN model would probably do a good job of filling in the missing values. This would find all the nearest points and impute the categorical value that is most often present in these nearest points. See example below: #https://cran.r-project.org/web/packages/VIM/VIM.pdf loadPkg(&quot;VIM&quot;) #show incomplete cases Soybean[which(!complete.cases(Soybean))[0:5],] ## Class date plant.stand precip temp hail crop.hist area.dam ## 32 phytophthora-rot 1 1 2 1 &lt;NA&gt; 3 1 ## 33 phytophthora-rot 2 1 2 2 &lt;NA&gt; 2 1 ## 35 phytophthora-rot 2 1 2 2 &lt;NA&gt; 2 1 ## 36 phytophthora-rot 3 1 2 1 &lt;NA&gt; 2 1 ## 39 phytophthora-rot 2 1 1 1 &lt;NA&gt; 0 1 ## sever seed.tmt germ plant.growth leaves leaf.halo leaf.marg leaf.size ## 32 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 1 1 0 2 2 ## 33 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 1 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 35 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 1 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 36 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 1 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 39 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 1 1 0 2 2 ## leaf.shread leaf.malf leaf.mild stem lodging stem.cankers canker.lesion ## 32 0 0 0 1 &lt;NA&gt; 2 2 ## 33 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 1 &lt;NA&gt; 3 2 ## 35 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 1 &lt;NA&gt; 2 2 ## 36 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 1 &lt;NA&gt; 3 2 ## 39 0 0 0 1 &lt;NA&gt; 2 2 ## fruiting.bodies ext.decay mycelium int.discolor sclerotia fruit.pods ## 32 &lt;NA&gt; 0 0 0 0 &lt;NA&gt; ## 33 &lt;NA&gt; 0 0 0 0 &lt;NA&gt; ## 35 &lt;NA&gt; 0 0 0 0 &lt;NA&gt; ## 36 &lt;NA&gt; 0 0 0 0 &lt;NA&gt; ## 39 &lt;NA&gt; 0 0 0 0 &lt;NA&gt; ## fruit.spots seed mold.growth seed.discolor seed.size shriveling roots ## 32 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 1 ## 33 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 1 ## 35 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 1 ## 36 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 1 ## 39 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 1 #impute Soybean_impute &lt;- kNN(Soybean, useImputedDist = FALSE, imp_var = FALSE) Soybean_impute[which(!complete.cases(Soybean))[0:5],] ## Class date plant.stand precip temp hail crop.hist area.dam ## 32 phytophthora-rot 1 1 2 1 0 3 1 ## 33 phytophthora-rot 2 1 2 2 0 2 1 ## 35 phytophthora-rot 2 1 2 2 0 2 1 ## 36 phytophthora-rot 3 1 2 1 0 2 1 ## 39 phytophthora-rot 2 1 1 1 0 0 1 ## sever seed.tmt germ plant.growth leaves leaf.halo leaf.marg leaf.size ## 32 2 1 1 1 1 0 2 2 ## 33 2 1 1 1 1 0 2 2 ## 35 2 1 1 1 1 0 2 2 ## 36 2 1 1 1 1 0 2 2 ## 39 2 1 0 1 1 0 2 2 ## leaf.shread leaf.malf leaf.mild stem lodging stem.cankers canker.lesion ## 32 0 0 0 1 0 2 2 ## 33 0 0 0 1 0 3 2 ## 35 0 0 0 1 0 2 2 ## 36 0 0 0 1 0 3 2 ## 39 0 0 0 1 0 2 2 ## fruiting.bodies ext.decay mycelium int.discolor sclerotia fruit.pods ## 32 0 0 0 0 0 3 ## 33 0 0 0 0 0 3 ## 35 0 0 0 0 0 3 ## 36 0 0 0 0 0 3 ## 39 0 0 0 0 0 3 ## fruit.spots seed mold.growth seed.discolor seed.size shriveling roots ## 32 4 0 0 0 0 0 1 ## 33 4 0 0 0 0 0 1 ## 35 4 0 0 0 0 0 1 ## 36 4 1 0 0 1 0 1 ## 39 4 0 0 0 0 0 1 Finally, it might make more sense to combine these two approaches and impute a value that indicates that the value was missing. That is, we treat a missing value as a category of its own. Imputing a “-1” would work fine as the predictors are all categorical, and as long as we are not using the ordinality in the factors, a “-1” value won’t mess up the model. This also avoids the need to have additional predictors and preserves the information that a missing value contains within the data set. #impute -1 Soybean_impute_Neg1 &lt;- Soybean Soybean_impute_Neg1 &lt;- data.frame(lapply(Soybean_impute_Neg1, as.character), stringsAsFactors = FALSE) Soybean_impute_Neg1[is.na(Soybean_impute_Neg1)] &lt;- &quot;-1&quot; #compare Soybean_impute_Neg1[which(!complete.cases(Soybean))[0:5],] ## Class date plant.stand precip temp hail crop.hist area.dam ## 32 phytophthora-rot 1 1 2 1 -1 3 1 ## 33 phytophthora-rot 2 1 2 2 -1 2 1 ## 35 phytophthora-rot 2 1 2 2 -1 2 1 ## 36 phytophthora-rot 3 1 2 1 -1 2 1 ## 39 phytophthora-rot 2 1 1 1 -1 0 1 ## sever seed.tmt germ plant.growth leaves leaf.halo leaf.marg leaf.size ## 32 -1 -1 -1 1 1 0 2 2 ## 33 -1 -1 -1 1 1 -1 -1 -1 ## 35 -1 -1 -1 1 1 -1 -1 -1 ## 36 -1 -1 -1 1 1 -1 -1 -1 ## 39 -1 -1 -1 1 1 0 2 2 ## leaf.shread leaf.malf leaf.mild stem lodging stem.cankers canker.lesion ## 32 0 0 0 1 -1 2 2 ## 33 -1 -1 -1 1 -1 3 2 ## 35 -1 -1 -1 1 -1 2 2 ## 36 -1 -1 -1 1 -1 3 2 ## 39 0 0 0 1 -1 2 2 ## fruiting.bodies ext.decay mycelium int.discolor sclerotia fruit.pods ## 32 -1 0 0 0 0 -1 ## 33 -1 0 0 0 0 -1 ## 35 -1 0 0 0 0 -1 ## 36 -1 0 0 0 0 -1 ## 39 -1 0 0 0 0 -1 ## fruit.spots seed mold.growth seed.discolor seed.size shriveling roots ## 32 -1 -1 -1 -1 -1 -1 1 ## 33 -1 -1 -1 -1 -1 -1 1 ## 35 -1 -1 -1 -1 -1 -1 1 ## 36 -1 -1 -1 -1 -1 -1 1 ## 39 -1 -1 -1 -1 -1 -1 1 I would try all of these approaches and select the one that works the best for the chosen models. "],
["hw4.html", "Chapter 5 HW4 5.1 7.5 5.2 7.6 5.3 7.10", " Chapter 5 HW4 5.1 7.5 5.1.1 7.5a Exploratory analysis Start with a simple line plot. books_dat &lt;- data.frame(Day = rep(1:30,times=2), Type = rep(c(&quot;Paperback&quot;,&quot;Hardcover&quot;),each=30), Sales = c(books[,1],books[,2]), stringsAsFactors=FALSE) ggplot(books_dat, aes(Day,Sales)) + geom_line() + facet_wrap(~Type) It feels like there could potentially be a day of the week pattern here, but it’s a bit hard to tell from this plot. Let’s plot each set of 7 days in its own panel. books_dat &lt;- data.frame(books_dat, Day.of.week = rep(c(rep(1:7,times=4),1,2),times=2), Week = paste0(&quot;Week&quot;,rep(c(rep(1:4,each=7),5,5),times=2)), stringsAsFactors=FALSE) ggplot(books_dat, aes(Day.of.week,Sales)) + geom_line() + geom_point() + facet_grid(Type ~ Week) + xlab(&quot;Day of the week&quot;) We don’t know here which actual day of the week from Sunday to Saturday each day of the week is, but we could infer if there were a weekday vs. weekend pattern if we saw two consecutive days consistently high/low. Or if there were one particular day of the week with higher sales for whatever reason, we would be able to see it from this plot. Here, it really does not seem like there is any pattern within what day of the week it is. As for trend, it seems like there could possibly be an increasing trend, though it’s admittedly a bit hard to tell with such strong daily fluctuations. For our forecasting, we’ll move forward based on the assumption that there is no trend and just run simple exponential smoothing. 5.1.2 7.5b Forecasting with ses Use the ses function to forecast. paperback_forecast &lt;- ses(books[,1],h=4) hardcover_forecast &lt;- ses(books[,2],h=4) autoplot(paperback_forecast) + xlab(&quot;Day&quot;) + ylab(&quot;Sales&quot;) + ggtitle(&quot;Paperback sales from past 30 days\\n+ forecast through next 4 days (with confidence intervals)&quot;) autoplot(hardcover_forecast) + xlab(&quot;Day&quot;) + ylab(&quot;Sales&quot;) + ggtitle(&quot;Hardcover sales from past 30 days\\n+ forecast through next 4 days (with confidence intervals)&quot;) Print the numbers for each. ## [1] &quot;Paperback forecast:&quot; paperback_forecast ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 31 207.1097 162.4882 251.7311 138.8670 275.3523 ## 32 207.1097 161.8589 252.3604 137.9046 276.3147 ## 33 207.1097 161.2382 252.9811 136.9554 277.2639 ## 34 207.1097 160.6259 253.5935 136.0188 278.2005 ## [1] &quot;Hardcover forecast:&quot; hardcover_forecast ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 31 239.5601 197.2026 281.9176 174.7799 304.3403 ## 32 239.5601 194.9788 284.1414 171.3788 307.7414 ## 33 239.5601 192.8607 286.2595 168.1396 310.9806 ## 34 239.5601 190.8347 288.2855 165.0410 314.0792 5.1.3 7.5c Compute RMSE values for the training data in each case. The MSE (mean squared error) is already included in “mse” within the “model” from the forecast object. Just take the square root of this to get the RSME (root mean squared error). ## [1] &quot;RMSE paperback forecast:&quot; sqrt(paperback_forecast$model$mse) ## [1] 33.63769 ## [1] &quot;RMSE hardcover forecast:&quot; sqrt(hardcover_forecast$model$mse) ## [1] 31.93101 The RSME can be thought of as something similar to the standard deviation for the residuals. Basically, it’s a way to measure the spread of how much the actual values deviate from the predictions of the model within the training data. Here, both paperback and hardcover forecasts have relatively similar RSME values of ~32-34. If the residuals are normally distributed, this means we would expect most predictions to fall within +/- 32-34 books vs. the actual value. While the vast majority would fall within +/- 64-68 books. 5.2 7.6 5.2.1 7.6a Apply Holt’s linear method to both series and compute four-day forecasts in each case. paperback_holt &lt;- holt(books[,1],h=4) hardcover_holt &lt;- holt(books[,2],h=4) autoplot(paperback_holt) + xlab(&quot;Day&quot;) + ylab(&quot;Sales&quot;) + ggtitle(&quot;Paperback sales from past 30 days\\n+ Holt&#39;s linear trend forecast through next 4 days&quot;) autoplot(hardcover_holt) + xlab(&quot;Day&quot;) + ylab(&quot;Sales&quot;) + ggtitle(&quot;Hardcover sales from past 30 days\\n+ Holt&#39;s linear trend forecast through next 4 days&quot;) ## [1] &quot;Paperback forecast using Holt&#39;s linear trend:&quot; paperback_holt ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 31 209.4668 166.6035 252.3301 143.9130 275.0205 ## 32 210.7177 167.8544 253.5811 145.1640 276.2715 ## 33 211.9687 169.1054 254.8320 146.4149 277.5225 ## 34 213.2197 170.3564 256.0830 147.6659 278.7735 ## [1] &quot;Hardcover forecast using Holt&#39;s linear trend:&quot; hardcover_holt ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 31 250.1739 212.7390 287.6087 192.9222 307.4256 ## 32 253.4765 216.0416 290.9113 196.2248 310.7282 ## 33 256.7791 219.3442 294.2140 199.5274 314.0308 ## 34 260.0817 222.6468 297.5166 202.8300 317.3334 5.2.2 7.6b Compare RSME measures vs. simple exponential smoothing. First, calculate RSME for Holt’s forecasts. ## [1] &quot;RSME for Holt&#39;s on paperback&quot; sqrt(paperback_holt$model$mse) ## [1] 31.13692 ## [1] &quot;RMSE for Holt&#39;s on hardcover&quot; sqrt(hardcover_holt$model$mse) ## [1] 27.19358 The RMSE error values using Holt’s linear trend is lower for both series than using simple exponential smoothing. Assuming both methods have a mean of the residuals around 0, this would mean that we should prefer Holt’s linear trend, since its predictions show less deviation from being perfectly accurate. However, this is a big assumption! Let’s check if it’s true. ## [1] &quot;Means of residuals for SES, then Holt&#39;s, for paperback series:&quot; mean(paperback_forecast$residuals) ## [1] 7.175981 mean(paperback_holt$residuals) ## [1] -3.717178 ## [1] &quot;Means of residuals for SES, then Holt&#39;s, for hardcover series:&quot; mean(hardcover_forecast$residuals) ## [1] 9.166735 mean(hardcover_holt$residuals) ## [1] -0.1357882 Looks like the mean of the residuals using Holt’s is actually closer to 0 than for SES, so that’s another check in favor of Holt’s. Finally, let’s visualize the distribution of residuals as well as plot time vs. residuals. par(mfrow=c(2,2)) hist_residuals &lt;- function(residuals,title){ hist(residuals, xlab=&quot;Residuals&quot;, ylab=&quot;Number of predictions&quot;, labels=TRUE, breaks=seq(from=-80,to=80,by=20), main=title) } hist_residuals(paperback_forecast$residuals, &quot;Paperback, SES&quot;) hist_residuals(paperback_holt$residuals, &quot;Paperback, Holt&#39;s linear&quot;) hist_residuals(hardcover_forecast$residuals, &quot;Hardcover, SES&quot;) hist_residuals(hardcover_holt$residuals, &quot;Hardcover, Holt&#39;s linear&quot;) residuals_vs_time &lt;- function(residuals,title){ plot(1:30, residuals, xlab=&quot;Day&quot;, ylab=&quot;Residuals&quot;, type=&quot;p&quot;, main=title, ylim=c(-70,80)) abline(h=0,lty=2) } par(mfrow=c(2,2)) residuals_vs_time(paperback_forecast$residuals, &quot;Paperback, SES&quot;) residuals_vs_time(paperback_holt$residuals, &quot;Paperback, Holt&#39;s linear&quot;) residuals_vs_time(hardcover_forecast$residuals, &quot;Hardcover, SES&quot;) residuals_vs_time(hardcover_holt$residuals, &quot;Hardcover, Holt&#39;s linear&quot;) The distributions of residuals look pretty similar. It’s hard to tell with only 30 data points, but they all seem relatively close to normal. There does not seem to be any major bias across time for predictions to be in any one direction. Therefore with these other factors being equal and RSME being lower using Holt’s linear trend, it seems that Holt’s linear trend has the clear advantage if we consider only performance within the training data. 5.2.3 7.6c Compare the forecasts for the two series using both methods. Which do you think is best? We decided in part 2 that Holt’s linear trend would definitely be considered best if we looked only at performance within the training data. Now, we are asked to also compare them using the forecasts into the future. We saw above that SES gives the same forecast value for all four days into the future, with only the prediction intervals changing over time. Whereas Holt’s linear trend gives a different (in this case increasing) value for each point further out in time. Paperback SES gave a prediction of 207.1097 books for all four days. Meanwhile Holt’s method gave a prediction pretty close to the prediction from SES (209.4668 books) on the first day. But by the last day its prediction was decently higher (213.2197 books). Similar idea for the hardcover series (239.5601 from SES, 250.1739 to 260.0817 from Holt’s linear method). Let’s look at how many days had sales higher than these different predictions within each series. ## [1] &quot;Days with sales higher than SES prediction, then first day Holt&#39;s prediction, then last day Holt&#39;s prediction, paperback series:&quot; length(which(books[,1] &gt; paperback_forecast$mean[1])) ## [1] 8 length(which(books[,1] &gt; paperback_holt$mean[1])) ## [1] 7 length(which(books[,1] &gt; paperback_holt$mean[4])) ## [1] 7 ## [1] &quot;Same for hardcover series:&quot; length(which(books[,2] &gt; hardcover_forecast$mean[1])) ## [1] 5 length(which(books[,2] &gt; hardcover_holt$mean[1])) ## [1] 3 length(which(books[,2] &gt; hardcover_holt$mean[4])) ## [1] 2 We find that both SES and Holt’s method predict future sales that are higher than most sales of the past 30 days, for both series. So while the predictions using Holt’s method are even higher than for SES, it’s not like they are that much more out of the realm than the predictions from SES. With this information combined with the better performance within the training data, I would continue to say that Holt’s linear trend is the better choice over SES for forecasting from this data. 5.2.4 7.6d Calculate prediction intervals and compare to intervals from SES and Holt functions. First, let’s review the intervals from SES and Holt functions. Prediction interval for first forecast from SES, paperback series: 138.8670-275.3523 (range ~136) Holt’s linear trend, paperback series: 143.9130-275.0205 (range ~131) SES, hardcover series: 174.7799-304.3403 (range ~130) Holt’s linear trend, hardcover series: 192.9222-307.4256 (range ~115) Now, compare to what we would get from calculating it ourselves using RMSE and assumption of a normal distribution. zscore_for_95percent_interval &lt;- qnorm(.975) ## [1] &quot;Intervals for SES then Holt&#39;s, paperback series:&quot; plus_minus_interval &lt;- sqrt(paperback_forecast$model$mse)*zscore_for_95percent_interval c(paperback_forecast$mean[1] - plus_minus_interval,paperback_forecast$mean[1] + plus_minus_interval) ## [1] 141.1810 273.0383 plus_minus_interval &lt;- sqrt(paperback_holt$model$mse)*zscore_for_95percent_interval c(paperback_holt$mean[1] - plus_minus_interval,paperback_holt$mean[1] + plus_minus_interval) ## [1] 148.4395 270.4940 ## [1] &quot;Intervals for SES then Holt&#39;s, hardcover series:&quot; plus_minus_interval &lt;- sqrt(hardcover_forecast$model$mse)*zscore_for_95percent_interval c(hardcover_forecast$mean[1] - plus_minus_interval,hardcover_forecast$mean[1] + plus_minus_interval) ## [1] 176.9765 302.1437 plus_minus_interval &lt;- sqrt(hardcover_holt$model$mse)*zscore_for_95percent_interval c(hardcover_holt$mean[1] - plus_minus_interval,hardcover_holt$mean[1] + plus_minus_interval) ## [1] 196.8754 303.4723 To summarize again: SES, paperback: 138.8670-275.3523 from function vs. 141.1810-273.0383 calculated Holt’s, paperback: 143.9130-275.0205 vs. 148.4395-270.4940 SES, hardcover: 174.7799-304.3403 vs. 176.9765-302.1437 Holt’s, hardcover: 192.9222-307.4256 vs. 196.8754-303.4723 All of our calculated intervals are very slightly narrower than those given by the forecasting functions. Overall though, they are quite similar. 5.3 7.10 5.3.1 7.10a Start with a simple line plot. autoplot(ukcars) + ggtitle(&quot;1977 to Q1 2005&quot;) Also look at each quarter separately. ggsubseriesplot(ukcars) + ggtitle(&quot;Yearly performance from 1977 to 2004/2005, separated by quarter&quot;) There is a clear seasonal component to the data, with lower sales on average in Q3. There also appears to be an increasing trend, at least from around 1981-2000 or so. 5.3.2 7.10b Let’s use the stl function to do this, and plot the result. stl_decomp &lt;- stl(ukcars,s.window=&quot;periodic&quot;) autoplot(stl_decomp) + ggtitle(&quot;STL decomposition of ukcars&quot;) These results look pretty sensible. We can clearly see the dips in Q3 in the seasonal component, and the increasing trend throughout most of the time span in the trend component. 5.3.3 7.10c As described in the question, use the function stlf() to do this. Then, plot the seasonally adjusted data and corresponding forecast. damped_forecast_after_stl &lt;- stl_decomp %&gt;% seasadj() %&gt;% stlf(etsmodel=&quot;AAN&quot;, damped=TRUE,h=2) autoplot(damped_forecast_after_stl) + xlab(&quot;Time&quot;) + ylab(&quot;Seasonally adjusted production&quot;) 5.3.4 7.10d non_damped_forecast_after_stl &lt;- stl_decomp %&gt;% seasadj() %&gt;% stlf(etsmodel=&quot;AAN&quot;, damped=FALSE,h=2) autoplot(non_damped_forecast_after_stl) + xlab(&quot;Time&quot;) + ylab(&quot;Seasonally adjusted production&quot;) 5.3.5 7.10e ets_model &lt;- ets(ukcars) ets_model$components ## [1] &quot;A&quot; &quot;N&quot; &quot;A&quot; &quot;FALSE&quot; The third letter denotes the season type. Here we get “A”, aka an additive seasonal model. 5.3.6 7.10f ## [1] &quot;RSME damped trend after STL decomposition, non-damped after STL decomposition, and ETS automated model selection:&quot; sqrt(damped_forecast_after_stl$model$mse) ## [1] 23.32113 sqrt(non_damped_forecast_after_stl$model$mse) ## [1] 23.295 sqrt(ets_model$mse) ## [1] 25.23244 Based on RSME, both models after STL decomposition perform better on the training data than the one using non-adjusted data and the ETS function. The non-damped performs slightly better than the damped, but they are really extremely close. 5.3.7 7.10g ## [1] &quot;Forecasts damped trend after STL decomposition, non-damped after STL decomposition, and ETS automated model selection:&quot; damped_forecast_after_stl ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 2005 Q2 394.0146 363.4433 424.5858 347.2599 440.7692 ## 2005 Q3 409.3791 371.9947 446.7634 352.2047 466.5535 non_damped_forecast_after_stl ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 2005 Q2 395.1772 364.7806 425.5738 348.6897 441.6648 ## 2005 Q3 411.4591 373.9155 449.0027 354.0411 468.8771 ets_forecast &lt;- forecast(ets_model,h=2) ets_forecast ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 2005 Q2 427.4885 394.2576 460.7195 376.6662 478.3109 ## 2005 Q3 361.3329 322.2353 400.4305 301.5383 421.1275 The damped forecast has slightly lower forecast point values than the non-damped, as we would expect. The ETS model predicts much higher Q2 production and much lower Q3 production than the other two models. However, it is hard to compare directly because the ETS model is based on non-adjusted data, while the others are based on adjusted data. To convert the forecasts after STL decomposition to real-world realistic numbers, we would have to add back in the seasonal component to these forecasts. 5.3.8 7.10h Let’s make a histogram and plot time vs. residuals for the ETS model. par(mfrow=c(1,2)) hist(ets_model$residuals, labels=TRUE, xlab=&quot;Residuals&quot;, ylab=&quot;Number of quarters&quot;, main=&quot;ETS model&quot;) mycol &lt;- c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;,&quot;#490092&quot;) plot(c(rep(1977:2004,each=4),2005), ets_model$residuals, xlab=&quot;Time&quot;, ylab=&quot;Residuals&quot;, main=&quot;ETS model&quot;, col=c(rep(mycol[1:4],times=length(1977:2004)),mycol[1])) abline(h=0,lty=2) legend(&quot;bottom&quot;, legend=paste0(&quot;Q&quot;,1:4), col=mycol, lwd=3,bty=&quot;n&quot;) Residuals appear to be normally distributed and unbiased over time and season. "],
["hw5.html", "Chapter 6 HW5", " Chapter 6 HW5 6.0.1 8.1 6.0.1.1 8.1a Answer: An ACF plot shows the correlations from linear relationships between lagged values of a time series for various lags. The first plot has the largest magnitude of correlations amongst lagged values, with some getting to about 0.25 and -0.3. It also has the largest critical values (the blue lines). As we move to the right, the critical values decrease and the correlations also decrease in magnitude until in the third chart, one can hardly see the correlations. I think that the charts pretty well show that the data are white noise. We don’t see any correlations far over the critical value lines so that suggests there aren’t correlations from one value to the next (meaning that it is in reasonable to assume that it is white noise). 6.0.1.2 8.1b Answer: In the first chart, the sample size is so small (36), and the chances of an accidental correlation from that small sample are so great, that the correlation would have to be very high in order to be significant. This is why the critical values are pretty large in magnitude (around 0.3 and -0.3). As the number of values in each sample increases and we move to the right, the likelihood of getting an accidental correlation due purely to random luck decreases, so the critical values showing the bar for significance also decrease as even a very modest correlation is likely to be significant and not due purely to random luck. Furthermore, the autocorrelations also decrease in magnitude as we move from left to right, as there are more and more examples of the relationship between one value and the next. When there are few values, it is very easy to find a correlation between subsequent values based solely on random luck coming from white noise. However, as more values are added, it becomes more difficult to find a correlation that is due to luck. Not finding a strong relationship, the correlation values decrease. 6.0.2 8.2 Answer: We plot the data below. Just by looking at it we can tell that there are instances of trend and of decreasing variance. Consequently, we know that it is not stationary. library(fpp2) autoplot(ibmclose) If we look at the ACF, we can see that each value is related to previous values. At each increasing lag value, there is still great signficance in the relationship between a value and the related lag value. This means that the time at which a value occurs is important to determining its value, so again, it is non-stationary. In terms of difference, every lag value is significant. What we don’t know if whether the significance of lag 2, for example, comes from its relationshop to lag 1, or if is is significant on its own. Thats why we need to look at the PACF. #ACF acf(ibmclose) The PACF removes the significance due to previous lags. This chart clearly shows that the lag of 1 is the significant lag, and any subsequent lags are significant only because they had the relationship with the first lag. So a difference of 1 is the correct difference to be applied here. #PACF pacf(ibmclose) 6.0.3 8.7 6.0.3.1 8.7a wmurders %&gt;% autoplot() wmurders %&gt;% diff() %&gt;%autoplot() wmurders %&gt;% diff(differences = 2) %&gt;%autoplot() It looks like a random walk, though neither the difference nor the second order difference look like white noise. wmurders %&gt;% ggAcf() wmurders %&gt;% diff() %&gt;% ggAcf() wmurders %&gt;% diff(differences = 2) %&gt;% ggAcf() The data ACF is decreasing linearly which indicates a non-stationary time series. There difference is one lag above significance, we can use the Box-Ljung test to see if it is indeed significant. However, is stationary as the lags radiply decrease under the significance thershold. Two differences give two significant lags, but again rapidly decreases to zero. wmurders %&gt;% diff() %&gt;% Box.test(lag=10, type=&quot;Ljung-Box&quot;) ## ## Box-Ljung test ## ## data: . ## X-squared = 12.958, df = 10, p-value = 0.226 wmurders %&gt;% diff(differences = 2) %&gt;% Box.test(lag=10, type=&quot;Ljung-Box&quot;) ## ## Box-Ljung test ## ## data: . ## X-squared = 40.778, df = 10, p-value = 1.235e-05 The Box-Ljung test shows the first order difference is white noise to the 95% confidence level. library(urca) wmurders %&gt;% ur.kpss() %&gt;% summary() ## ## ####################### ## # KPSS Unit Root Test # ## ####################### ## ## Test is of type: mu with 3 lags. ## ## Value of test-statistic is: 0.6331 ## ## Critical value for a significance level of: ## 10pct 5pct 2.5pct 1pct ## critical values 0.347 0.463 0.574 0.739 wmurders %&gt;% diff() %&gt;% ur.kpss() %&gt;% summary() ## ## ####################### ## # KPSS Unit Root Test # ## ####################### ## ## Test is of type: mu with 3 lags. ## ## Value of test-statistic is: 0.4697 ## ## Critical value for a significance level of: ## 10pct 5pct 2.5pct 1pct ## critical values 0.347 0.463 0.574 0.739 wmurders %&gt;% diff(differences = 2) %&gt;% ur.kpss() %&gt;% summary() ## ## ####################### ## # KPSS Unit Root Test # ## ####################### ## ## Test is of type: mu with 3 lags. ## ## Value of test-statistic is: 0.0458 ## ## Critical value for a significance level of: ## 10pct 5pct 2.5pct 1pct ## critical values 0.347 0.463 0.574 0.739 wmurders %&gt;% ndiffs() ## [1] 2 Both by the KPSS test and ndiff(), 2 differences seem to give the best result. The KPSS test for 1 difference is marginally stationary as the test statstic is 0.0067 above the 95% confidence level. 6.0.3.2 8.7b wmurders %&gt;% mean() ## [1] 3.392687 wmurders %&gt;% diff() %&gt;% mean() ## [1] 0.00296237 There does not seem to be drift to the Random walk, so c = 0. There should not be a consant added. 6.0.3.3 8.7c \\[ (1 - \\phi_1B)(1-B)^2y_t = c + (1 - \\theta_1B)e_t \\] However it appears that the constant is zero as is the moving average part so: \\[ (1 - \\phi_1B)(1-B)^2y_t = (1 - \\theta_1B)e_t \\] 6.0.3.4 8.7d It seems like a 1,1,1 or 1,2,1 model may be the best, but we will test against some similar models to see how they perform. orders &lt;- matrix(c(c(1,0,0),c(0,1,0),c(0,0,1),c(1,1,0),c(1,0,1),c(0,1,1),c(1,1,1),c(2,1,1),c(1,2,1), c(1,1,2)), nrow = 10, byrow = TRUE) orders ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 ## [4,] 1 1 0 ## [5,] 1 0 1 ## [6,] 0 1 1 ## [7,] 1 1 1 ## [8,] 2 1 1 ## [9,] 1 2 1 ## [10,] 1 1 2 for(row in 1:nrow(orders)){ fit &lt;- wmurders %&gt;% Arima(order = orders[row,]) checkresiduals(fit) print(fit$aicc) } ## ## Ljung-Box test ## ## data: Residuals from ARIMA(1,0,0) with non-zero mean ## Q* = 12.887, df = 8, p-value = 0.1158 ## ## Model df: 2. Total lags used: 10 ## ## [1] -6.095071 ## ## Ljung-Box test ## ## data: Residuals from ARIMA(0,1,0) ## Q* = 13.165, df = 10, p-value = 0.2146 ## ## Model df: 0. Total lags used: 10 ## ## [1] -11.38057 ## ## Ljung-Box test ## ## data: Residuals from ARIMA(0,0,1) with non-zero mean ## Q* = 153.81, df = 8, p-value &lt; 2.2e-16 ## ## Model df: 2. Total lags used: 10 ## ## [1] 81.0769 ## ## Ljung-Box test ## ## data: Residuals from ARIMA(1,1,0) ## Q* = 13.281, df = 9, p-value = 0.1503 ## ## Model df: 1. Total lags used: 10 ## ## [1] -9.610749 ## ## Ljung-Box test ## ## data: Residuals from ARIMA(1,0,1) with non-zero mean ## Q* = 12.918, df = 7, p-value = 0.07414 ## ## Model df: 3. Total lags used: 10 ## ## [1] -3.93281 ## ## Ljung-Box test ## ## data: Residuals from ARIMA(0,1,1) ## Q* = 13.122, df = 9, p-value = 0.1572 ## ## Model df: 1. Total lags used: 10 ## ## [1] -9.46505 ## ## Ljung-Box test ## ## data: Residuals from ARIMA(1,1,1) ## Q* = 11.266, df = 8, p-value = 0.1871 ## ## Model df: 2. Total lags used: 10 ## ## [1] -9.876999 ## ## Ljung-Box test ## ## data: Residuals from ARIMA(2,1,1) ## Q* = 9.4234, df = 7, p-value = 0.2237 ## ## Model df: 3. Total lags used: 10 ## ## [1] -10.16966 ## ## Ljung-Box test ## ## data: Residuals from ARIMA(1,2,1) ## Q* = 12.419, df = 8, p-value = 0.1335 ## ## Model df: 2. Total lags used: 10 ## ## [1] -6.389972 ## ## Ljung-Box test ## ## data: Residuals from ARIMA(1,1,2) ## Q* = 9.9439, df = 7, p-value = 0.1918 ## ## Model df: 3. Total lags used: 10 ## ## [1] -10.62525 It seems like an ARIMA(1,2,1) is best in terms of residual behavior and AICc. Some of the lower AICc values have worse residuals and ACF plots. 6.0.3.5 8.7e three_ahead &lt;- fit %&gt;% forecast(h=3) three_ahead ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 2005 2.450027 2.184288 2.715766 2.043615 2.856439 ## 2006 2.463104 2.098363 2.827846 1.905280 3.020928 ## 2007 2.463646 1.957435 2.969857 1.689463 3.237828 \\[ (1 - \\phi B)(1-B)^2y_t = (1 - \\theta B)e_t\\\\ \\phi_1 = 0.395 \\\\ \\phi_2 = -0.65 \\\\ \\phi_2 = 0.05 \\\\ \\theta_1 = 1 \\\\ y_t = \\phi_1y_{t-1} - \\phi_2 y_{t-2} + \\phi_3 y_{t-3} +\\epsilon_t + \\theta_1\\epsilon_{t-1} \\\\ y_t = 0.395y_{t-1} + 0.65y_{t-2} + 0.05 y_{t-3} +\\epsilon_t + \\epsilon_{t-1} \\] phi_1 = 0.395 phi_2 = 0.65 phi_3 = 0.005 theta_1 = 1 y_t_1 = wmurders[1] y_t_2 = wmurders[2] y_t_3 = wmurders[3] e_t = fit$residuals[1] e_t_1 =fit$residuals[2] y_2005 = phi_1*y_t_1 + phi_2*y_t_2 + phi_3*y_t_3 + e_t +theta_1*e_t_1 y_2005 %&gt;% print() ## [1] 2.448481 y_t_1 = y_2005 y_t_2 = wmurders[1] y_t_3 = wmurders[2] e_t = y_2005 - wmurders[1] #error from naive model estimate e_t_1 = fit$residuals[1] y_2006 = phi_1*y_t_1 + phi_2*y_t_2 + phi_3*y_t_3 + e_t +theta_1*e_t_1 y_2006 %&gt;% print() ## [1] 2.579583 y_t_1 = y_2006 y_t_2 = y_2005 y_t_3 = wmurders[1] e_t = y_2006 - y_2005 #error from naive model estimate e_t_1 = y_2005 - wmurders[1] y_2007 = phi_1*y_t_1 + phi_2*y_t_2 + phi_3*y_t_3 + e_t +theta_1*e_t_1 y_2007 %&gt;% print() ## [1] 2.772763 Our by-hand method of the (1,2,1) model gives a steeper upward trend than the included method. The challenge for matching the by-hand model to the R fucntion is balancing the parameters of \\(\\phi_{1,2,3}\\) and \\(\\theta_1\\). This was done emptically and it became appearant that small changes to the paramters made big changes in the predictions as given below: phi_1 = 0.5 phi_2 = 0.495 phi_3 = 0.005 theta_1 = 1 y_t_1 = wmurders[1] y_t_2 = wmurders[2] y_t_3 = wmurders[3] e_t = fit$residuals[1] e_t_1 =fit$residuals[2] y_2005 = phi_1*y_t_1 + phi_2*y_t_2 + phi_3*y_t_3 + e_t +theta_1*e_t_1 y_2005 %&gt;% print() ## [1] 2.337249 y_t_1 = y_2005 y_t_2 = wmurders[1] y_t_3 = wmurders[2] e_t = y_2005 - wmurders[1] #error from naive model estimate e_t_1 = fit$residuals[1] y_2006 = phi_1*y_t_1 + phi_2*y_t_2 + phi_3*y_t_3 + e_t +theta_1*e_t_1 y_2006 %&gt;% print() ## [1] 2.293265 y_t_1 = y_2006 y_t_2 = y_2005 y_t_3 = wmurders[1] e_t = y_2006 - y_2005 #error from naive model estimate e_t_1 = y_2005 - wmurders[1] y_2007 = phi_1*y_t_1 + phi_2*y_t_2 + phi_3*y_t_3 + e_t +theta_1*e_t_1 y_2007 %&gt;% print() ## [1] 2.179567 Here shift the weight from \\(\\phi_2\\) to \\(\\phi_1\\) by 0.105 and instead of getting an increasing trend we get a decreasing trend. The first example respresents our best attempt to constrain the \\(\\phi\\) parameters empirically. 6.0.3.6 8.7f fit %&gt;% forecast(h=3) %&gt;% autoplot() Does auto.arima() give the same model you have chosen? If not, which model do you think is better? auto.arima(wmurders) ## Series: wmurders ## ARIMA(1,2,1) ## ## Coefficients: ## ar1 ma1 ## -0.2434 -0.8261 ## s.e. 0.1553 0.1143 ## ## sigma^2 estimated as 0.04632: log likelihood=6.44 ## AIC=-6.88 AICc=-6.39 BIC=-0.97 It give the same answer. 6.0.3.7 8.7g 6.0.4 8.12 data(mcopper) autoplot(mcopper) 6.0.4.1 8.12a Answer: given the huge spike in 2005-2010 in copper prices, a Box-Cox transformation to help with the increased variance would be useful. A Box-Cox test finds a lambda of 0.191 to be optimal. lambda &lt;-BoxCox.lambda(mcopper) mcopper_adj &lt;- BoxCox(mcopper,lambda) autoplot(mcopper_adj) 6.0.4.2 8.12b Answer: auto.arima() finds an ARIMA(0,1,1) model, meaning that we have a 0 autoregressive part, with a single first degree difference, and 1 for the moving average part. model &lt;- auto.arima(mcopper_adj) summary(model) ## Series: mcopper_adj ## ARIMA(0,1,1) ## ## Coefficients: ## ma1 ## 0.3720 ## s.e. 0.0388 ## ## sigma^2 estimated as 0.04997: log likelihood=45.05 ## AIC=-86.1 AICc=-86.08 BIC=-77.43 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 0.01254827 0.2231365 0.1592271 0.08049384 1.140225 0.1997301 ## ACF1 ## Training set -0.004184621 6.0.4.3 8.12c Answer: I tried six other options with different orders. All were either equal or less in log likelihood or had higher AIC and AICc. They were also more complicated by adding, for example, an autoregressive part when the above model doesn’t have one. option1 &lt;-Arima(mcopper_adj, order=c(1,0,1)) summary(option1) ## Series: mcopper_adj ## ARIMA(1,0,1) with non-zero mean ## ## Coefficients: ## ar1 ma1 mean ## 0.9955 0.3740 14.1730 ## s.e. 0.0044 0.0388 2.1536 ## ## sigma^2 estimated as 0.05009: log likelihood=42.77 ## AIC=-77.55 AICc=-77.48 BIC=-60.21 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 0.01102121 0.2232071 0.1600608 0.05878252 1.148836 0.2007759 ## ACF1 ## Training set -0.003909071 option2 &lt;-Arima(mcopper_adj, order=c(1,1,0)) summary(option2) ## Series: mcopper_adj ## ARIMA(1,1,0) ## ## Coefficients: ## ar1 ## 0.3231 ## s.e. 0.0399 ## ## sigma^2 estimated as 0.05091: log likelihood=39.83 ## AIC=-75.66 AICc=-75.64 BIC=-66.99 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 0.01154793 0.2252223 0.1601516 0.07483773 1.147554 0.2008897 ## ACF1 ## Training set 0.03831971 option3 &lt;-Arima(mcopper_adj, order=c(0,1,0)) summary(option3) ## Series: mcopper_adj ## ARIMA(0,1,0) ## ## sigma^2 estimated as 0.05674: log likelihood=8.85 ## AIC=-15.7 AICc=-15.69 BIC=-11.37 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 0.01733237 0.2379863 0.1716493 0.1074569 1.227148 0.2153122 ## ACF1 ## Training set 0.3187708 option4 &lt;-Arima(mcopper_adj, order=c(1,0,0)) summary(option4) ## Series: mcopper_adj ## ARIMA(1,0,0) with non-zero mean ## ## Coefficients: ## ar1 mean ## 0.9980 13.9151 ## s.e. 0.0025 3.0608 ## ## sigma^2 estimated as 0.05692: log likelihood=6.21 ## AIC=-6.41 AICc=-6.37 BIC=6.59 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 0.01677141 0.2381561 0.1721394 0.09649416 1.232327 0.2159269 ## ACF1 ## Training set 0.3189333 option5 &lt;-Arima(mcopper_adj, order=c(0,2,1)) summary(option5) ## Series: mcopper_adj ## ARIMA(0,2,1) ## ## Coefficients: ## ma1 ## -1.0000 ## s.e. 0.0066 ## ## sigma^2 estimated as 0.05664: log likelihood=6.66 ## AIC=-9.32 AICc=-9.3 BIC=-0.66 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 0.004015354 0.2373549 0.1711164 0.01596248 1.225196 0.2146437 ## ACF1 ## Training set 0.3187439 option6 &lt;-Arima(mcopper_adj, order=c(1,1,1)) summary(option6) ## Series: mcopper_adj ## ARIMA(1,1,1) ## ## Coefficients: ## ar1 ma1 ## -0.0092 0.3797 ## s.e. 0.1053 0.0961 ## ## sigma^2 estimated as 0.05006: log likelihood=45.05 ## AIC=-84.1 AICc=-84.06 BIC=-71.1 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 0.01259577 0.2231351 0.1592422 0.08076154 1.14026 0.199749 ## ACF1 ## Training set -0.002728724 6.0.4.4 8.12d Answer: for the reasons given above, the original model is the best. We can confirm this by also forcing auto.arima() to check more of the available options. It still returns an ARIMA(0,1,1) model. The residuals look good. A Ljung-Box test returns a p-value of 0.4659 which is not significant. The residuals are normally distributed, are not autocorrelated, and have roughly stable variance over time. So the model appears to pass the test. model2 &lt;- auto.arima(mcopper_adj, stepwise=FALSE, approximation=FALSE) summary(model2) ## Series: mcopper_adj ## ARIMA(0,1,1) ## ## Coefficients: ## ma1 ## 0.3720 ## s.e. 0.0388 ## ## sigma^2 estimated as 0.04997: log likelihood=45.05 ## AIC=-86.1 AICc=-86.08 BIC=-77.43 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 0.01254827 0.2231365 0.1592271 0.08049384 1.140225 0.1997301 ## ACF1 ## Training set -0.004184621 #residuals checkresiduals(model) ## ## Ljung-Box test ## ## data: Residuals from ARIMA(0,1,1) ## Q* = 22.913, df = 23, p-value = 0.4659 ## ## Model df: 1. Total lags used: 24 6.0.4.5 8.12e Answer: a forecast of the fitted model looks reasonable. The prediction interval seems to be wide enough to capture what, based on looking at the historical data, are the likely values to come. autoplot(forecast(model)) 6.0.4.6 8.12f Answer: The ARIMA model appears to be a better model. The prediction intervals are much smaller (the ETS intervals provide almost no real help in what the value could be). Comparing summary statistics, the ARIMA model has AIC of -86.1, AICc 0f -86.08, and BIC of -77.43. Other measures are RMSE (0.223), MAE (0.159), MAPE (1.14), and MASE (0.199). In contrast, the ETS model has very high AIC (1919), AICc (1919), and BIC (1945). Other measures are similar or higher: RMSE(0.233), MAE (0.166), MAPE(1.19), and MASE(0.208). Also, the ETS model does not account for some autocorrelation, as shown by the ACF plot below and as given by the Ljung-Box test p-value of basically 0 (meaning that the results are significant and we can conclude that there is autocorrelation). So the ETS model in this instance does not do very well at all in comparison to the ARIMA model, which is much more useful in its predictions and much more accurate with respect to model measures. model_ets&lt;-ets(mcopper_adj) autoplot(forecast(model_ets)) summary(model_ets) ## ETS(M,Ad,N) ## ## Call: ## ets(y = mcopper_adj) ## ## Smoothing parameters: ## alpha = 0.9971 ## beta = 0.26 ## phi = 0.8 ## ## Initial states: ## l = 9.9204 ## b = -0.2684 ## ## sigma: 0.0168 ## ## AIC AICc BIC ## 1919.482 1919.633 1945.492 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 0.008892997 0.2330528 0.1662713 0.06395868 1.19264 0.2085661 ## ACF1 ## Training set 0.1508258 checkresiduals(model_ets) ## ## Ljung-Box test ## ## data: Residuals from ETS(M,Ad,N) ## Q* = 80.996, df = 19, p-value = 1.252e-09 ## ## Model df: 5. Total lags used: 24 "]
]
